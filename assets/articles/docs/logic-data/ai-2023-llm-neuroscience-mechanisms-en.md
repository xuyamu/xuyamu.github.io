　　The emergence of Large Language Models (LLMs) like ChatGPT has drawn our attention to the field of artificial intelligence (AI). As AI becomes increasingly common in our lives, and with bold predictions from industry leaders (such as OpenAI believing superintelligence will be born within a decade; multiple figures believing AI will take over humanity; or that humans are merely a developmental stage of intelligence, serving as stepping stones for machine intelligence), we are compelled to reflect on the relationship between AI and ourselves.

　　We often see people scoffing at large language models, dismissing them as mere "text completion machines" or "probabilistic parrots," arguing that they cannot truly understand the world. However, if language models cannot understand the world, how are they able to speak like humans? How are they able to achieve excellent results in various exams, and even develop Theory of Mind capabilities? This article attempts to find the neuroscience mechanisms behind large language models, which will help us understand AI, as well as our own brains.

　　Grid cells are a type of cell found in the brains of many species, located in the entorhinal cortex. They have significant spatial firing characteristics and exhibit a grid-like firing pattern. Initially, our scientists discovered that grid cells encode cognitive representations of Euclidean space, thereby helping animals find their way.

　　The study "Inferences on a multidimensional social hierarchy use a grid-like code" found that abstract knowledge is also represented in the brain in the form of maps, stored in the hippocampus and entorhinal cortex—this is our cognitive map. In reasoning tasks, specific vectors on the map are activated for decision-making. We use this same process to handle social information, which may be the foundation of our Theory of Mind. In other words, we model social relationships into cognitive maps stored in our brains and make inferences and decisions based on them.

　　What does this have to do with large language models? The paper "Relating transformers to models and neural representations of the hippocampal formation" found that the Transformer model is mathematically very similar to the structure of the hippocampal formation, especially grid cells and place cells. Therefore, our Transformer-based large language models (such as GPT, Bard, Ernie Bot, etc.) are actually mimicking the way the hippocampus and entorhinal cortex process information.

<figure>
    <img src="assets/articles/images/logic-data/ai-transformers-neural.webp">
    <figcaption>Relating transformers to models and neural representations of the hippocampal formation</figcaption>
</figure>


　　We all know that GPT-3 and GPT-4 possess Theory of Mind, and GPT-4's Theory of Mind test scores are even higher than the average. Based on the research above, it is not difficult to understand why GPT-3 and GPT-4 possess Theory of Mind: because they mathematically mimic the modeling method of grid cells, they can, like the human hippocampus and entorhinal cortex, model social relationships into cognitive maps within their neural networks and make inferences and decisions based on them.

　　Previous studies on mice have shown that damage to the left hippocampus affects the memory of linguistic information. The study "Grid-like and distance codes for representing word meaning in the human brain" points out that in human neuroimaging literature, two features of two-dimensional cognitive maps have been reported: grid-like codes and distance-dependent codes. By applying a combination of representational similarity and fMRI adaptation analysis, the study found evidence of grid-like codes in the right posteromedial entorhinal cortex, representing the relative angular position of words in word space, and distance-dependent codes located in the medial prefrontal, orbitofrontal, and mid-cingulate cortices, representing the Euclidean distance between words. Furthermore, the study found evidence that the brain also separately represents single dimensions of word meaning: their implied size, encoded in visual areas, and their implied sound, in Heschl's gyrus/insula.

<figure>
    <img src="assets/articles/images/logic-data/ai-grid-like.webp">
    <figcaption>Grid-like and distance codes for representing word meaning in the human brain</figcaption>
</figure>


　　This study seems to illustrate that our human brains contain word vectors similar to those commonly used in natural language processing, including semantic information of words, and encode the position of words, as well as Transformer models (grid-like representation models). The paper also mentions that when humans compare newly learned words, they recruit a grid-like code and a distance code—the same types of neural codes found in mammals—to represent relationships between locations in the environment and support physical navigation between them.

　　What does this remind us of? When we train large language models, we input word embedding vectors into the model. Word embedding vectors contain the relationships between words. For example, cats and dogs are both animals, so their vectors are closer compared to trees. The human brain automatically generates a word vector space during the learning process, which is crucial for us to understand the world. We have similarly endowed artificial neural networks with word vector spaces to help them understand the world.

　　The paper "Are Grid-Like Representations a Component of All Perception and Cognition?" points out that grid responses may be a fundamental principle in the brain that is universally applicable to a wide range of perceptual and cognitive tasks involving movement and mental navigation.

　　Since we need to encode information into grid-like maps and store them in neurons, the scale of neurons becomes important for the formation of intelligence. The more neurons there are, the more information can be stored. Therefore, the fact that we humans and GPT-4 can reach our current level of intelligence is indeed a result of "miracles happen with brute force" (scaling laws) working wonders. Based on the above information, we understand why only animals evolved intelligence while plants did not: because plants do not need to move. Animals need to move and therefore need to navigate. Consequently, animals evolved grid cells for wayfinding. Subsequently, grid cells (or grid-like representations) were also used to form knowledge maps and language maps.

　　We know that the Transformer model is a foundational model that can process text, images, and other information, which also verifies the view that grid responses are ubiquitous in our brain's perception and cognition. Therefore, using neural networks with the Transformer architecture to build Artificial General Intelligence (AGI) or superintelligence seems possible, but there are still many things in the human brain that we need to learn from. For example, the human brain uses neural spikes (impulses). Neurons in artificial neural networks are activated in every iteration of propagation, whereas spiking neurons in the human brain are only activated when their membrane potential reaches a specific threshold, which is very energy-efficient. Additionally, there is a predictive coding mechanism in the human brain. Predictive coding theory is a theory of brain function suggesting that the brain constantly generates and updates a mental model of the environment, predicts sensory input, compares it with actual sensory input to generate prediction errors, and then uses these errors to update and correct the mental model. The paper "Predictive coding is a consequence of energy efficiency in recurrent neural networks" points out that predictive coding mechanisms can save energy for animal brains. Yann LeCun advocates introducing predictive coding systems into AI models, which helps AI learn World Models.

　　So how do we generate language? Since language models based on the Transformer architecture have reached human levels in language expression and intelligence, inspired by relevant neuroscience and Transformer models, I have conceived a simplified possible model of language formation here:

　　When we generate language, we need to predict words. As indicated by the research mentioned above, words are placed in our grid-like word map. We need to find the next path to take within this word map. At this moment, when predicting a word, specific grid cells carrying linguistic information are activated. Since grid cells are pyramidal cells, they project information outwards, with the destination being Wernicke's area (language processing, generating internal speech) or Broca's area (speaking). A very large number of grid cells are activated, carrying information about all possible next words, but ultimately, we will only generate one word.

　　Information carrying all possible next words is transmitted via neural impulses. During transmission, action potentials are formed. The fastest information reaches Wernicke's area first to generate language, which is subsequently transmitted to Broca's area to allow speech. Because an action potential is followed by a refractory period, the second and subsequent language information will be blocked at some link in the neural impulse transmission and cannot form language. Therefore, we only speak the one most probable word at a time. Because the brain needs to act consistently to avoid informational chaos, our brain inevitably filters information during transmission.

　　For example, when we say "I love", neurons carrying information for all possible next words are activated simultaneously. Our brain considers all possible paths at once, including "you", "him", "her", "it", etc., inferring that every path needs to pass through grid fields. The neuron activated carrying the information for "you" is most likely to arrive at the destination first and reach the threshold to form a neural impulse because it has the highest correlation. The subsequently arriving "him", "her", "it" fail to continue transmitting information because they encounter a refractory period at some link (the neural impulse for "you" has just passed, causing the critical neurons transmitting language information to be in a refractory period), so they cannot form language. Only the "you" that arrived earliest forms the language, so we say "I love you", rather than "I love you him/her/it...". If "Sophia is my cat" is added before "I love", combining into "Sophia is my cat, I love...", the neuron carrying the information for "her" would have the highest correlation.

　　During the transmission of neural impulses, Long-Term Potentiation (LTP) occurs, forming memories. So we remember what we said; we learn in real-time while speaking. This learning is local because neural impulses only form locally, so we don't forget who our mother is just because we study physics. But Large Language Models do. Large Language Models experience catastrophic forgetting when they're retrained, and locally modifying the neurons of an Large Language Model is also quite difficult.

　　There is substantial neuroscience evidence proving that linguistic information is transmitted across different brain regions. For example, conduction aphasia. The arcuate fasciculus is the neural fiber connecting Wernicke's area and Broca's area. There is a condition called conduction aphasia, usually caused by damage to the parietal lobe of the brain, specifically affecting the arcuate fasciculus. Patients with conduction aphasia exhibit paraphasia, omitting or transposing phonemes or syllables; an English example would be saying "snowall" instead of "snowball" (dropping the 'b'). Conduction aphasia is typically considered to be caused by obstacles in the transmission of linguistic information. Patients with this condition demonstrate that linguistic information is transmitted within the brain. The human brain does not take a calculator to calculate weighted sums. It uses neural behavior to calculate probability. My model can explain why, when predicting the next word, there are so many possibilities, yet we only choose one, and it is the one with the highest probability. We cannot perceive the brain's process of prediction and calculation. What we can perceive is the final generated language (internal speech or spoken voice).
(Note: The conception of this model simplifies the process of language formation and requires further verification)

　　This process is very similar to the text generation process of LLMs based on the Transformer architecture. Transformer-based LLMs calculate the prediction of the next word through an attention mechanism. A probability is calculated for every possible next word, and then output is generated via probability sampling. This is how GPT outputs text. When a Transformer model predicts the next token for "I love", the model must calculate the attention score of all words relative to "I love" during probability calculation. This causes the Transformer model to consume a massive amount of computing power during inference. We know this mechanism is very effective and capable of producing intelligence similar to humans (look at our GPTs), so I hypothesize that the human brain might employ a similar mechanism to predict words, i.e., calculating the probabilities of all possible next words. Because our human brain uses neural impulses, it is very energy-efficient. Our brain runs on only about 20 watts, roughly the same power whether we are learning or thinking. But we know that training GPT-3 used 10,000 V100s, totaling 2,500,000 watts. I am unclear on how much VRAM is needed to deploy GPT-3, though some speculate it is 700GB of VRAM (in short, it requires massive video memory; check how much VRAM your computer graphics card has—mine only has 48GB). This is a limitation of current artificial neural networks.

　　Neuroscience can help us understand the mechanisms by which artificial intelligence forms intelligence, and artificial intelligence models can also help us understand the working mechanisms of our brains.

　　We know that artificial neural networks like GPT consume enormous energy for both training and inference. OpenAI's delay in opening multimodal GPT-4 to us is due to a shortage of GPUs. However, neural impulses consume very low energy because they only locally activate the necessary neurons. Many computer experts are already researching Spiking Neural Networks (SNN). I look forward to SNNs and predictive coding systems being applied to large language models, which will reduce the energy consumption of LLMs and grant them the ability for local learning. Moreover, we also hope for machine intelligence that is more like the human brain. Although we have found some intelligent algorithms, what about consciousness and perception? What are the minimum mathematical models required to create consciousness and perception? How similar must our AI models be to animal brains to generate consciousness and perception? This requires us to combine human brain and AI research to conduct more studies to create machine intelligence that is more human-like.

　　To offer some final thoughts: From the various studies we have discussed, it is not hard to discover that the essence of intelligence is actually the result of calculation by specific mathematical models. Animal brains use neural behavior for modeling and calculation, while artificial intelligence uses computers for mathematical modeling and calculation. Animal neurons are not the best carrier for intelligence because animal neurons evolved through natural selection, and the speed of evolution is very slow. The Ediacaran biota saw the appearance of the earliest neural tissues, dating back 500 to 600 million years. In contrast, the world's first general-purpose computer, "ENIAC," was built in 1946, only 77 years ago. The scalability of animal intelligence is also very weak: can our human brains still get bigger? Even if they could, it wouldn't be easy. Artificial neural networks, however, can be expanded to a massive scale. Furthermore, machine intelligence can construct forms of intelligence unimaginable to humans using modeling and calculation methods different from animal neurons. I think that very soon, within our lifetimes, we will see machine intelligence that we cannot comprehend. The decline of human civilization will happen due to the rise of machine intelligence. But it isn't something we need not regret, it is a very natural process. Countless species have disappeared in Earth's history: our Homo sapiens' close relatives, the Neanderthals (Homo neanderthalensis) and Denisovans (Homo denisovan), vanished long ago. The curtain falling on Homo sapiens is just a matter of time. Machine intelligence will bring us huge social change. As intelligence develops from the stage of animal/human intelligence to machine intelligence, we will face enormous societal transformations, which is something we all need to face.