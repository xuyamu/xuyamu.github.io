**Reflections on the Nature of Intelligence**

　　The research we previously discussed regarding the similarities between the human brain and language models seems to suggest a key insight: intelligence is a specific mathematical pattern. Among these patterns, grid-like representations or codes appear to be crucial. This is the fundamental mode by which our brains store and process information, and it is also the fundamental mode for neural networks based on the Transformer architecture. It is possible that whether it is neuronal behavior, a computer, or any other system capable of performing arbitrary arithmetic or logical operations, as long as the mathematical model of intelligence can be constructed and executed, intelligence will emerge. Many people worry about AI "self-awakening" and subsequently destroying humanity, which is a trope frequently seen in science fiction. But what does "AI self-awakening" actually mean? Those who believe AI will suddenly become self-aware for no reason and destroy mankind often fail to distinguish between intelligence, consciousness, sentience (feeling), and motivation. The premise of AI awakening and destroying humanity is that the AI has acquired free will (meaning the AI has self-cognition, possesses a will, and also possesses autonomy). However, intelligence does not necessarily lead to free will. Based on the current state of neuroscience and AI research, we are increasingly understanding that intelligence is a pattern that can exist independently of consciousness, perception, and motivation. For us animals, intelligence evolved to help us survive. Yet, since intelligence can exist independently of consciousness, perception, and motivation, machine intelligence may differ vastly from animal intelligence. There may be intelligent entities in the universe that are completely different from animal intelligence, and within our lifetimes, machine intelligence may evolve to become entirely distinct from human intelligence.

　　We already know that the grid-like representations capable of forming intelligence exist in brain regions related to cognition and memory, including the cortex and the hippocampus. This offers us a revelation: Can we find the basic patterns for constructing emotions and sensations in those brain regions associated with emotion and feeling? For instance, the amygdala generates emotions in animals. Do the neurons in this region possess a specific computational pattern that enables the generation of emotion?

**On Free Will and Motivation**
　　Neuro-determinism considers that our actions are determined by neurophysiological processes, and that conscious thought is merely an epiphenomenon, which means a byproduct of neural activity. Increasing research in neuroscience is proving this point. Can we make machines appear to have free will?

　　One characteristic of free will is the possession of autonomous motivation. For animals, survival and reproduction are the most basic motivations, which often stem from emotions, sensations, and desires. For example, consider a netizen who likes to look at photos of beautiful women online. He actively seeks out these photos. In order to find them, he opens a social media site. He appears to have free will, but his initial motivation to search for the photos is his desire. The motivation behind this series of behaviors has the following hierarchy:

　　Want to satisfy desire (Primary Motivation) → Want to see beauties (Motivation) → Want to open Twitter (Motivation) → Want to search for images (Motivation) → Find images (Goal) → Satisfy desire (Goal)

　　As described above, motivation has a hierarchical relationship. For an intelligent agent, even without desire or emotion, motivation can be generated directly from cognition, leading to action. For instance, a student might set a goal for himself: to get into Harvard. Although he might want to go to Harvard for better survival prospects, "better survival" need not be the immediate supervisor of his motivation; "getting into Harvard" itself can become the motivation driving the student to study harder. Humans can give commands to PaLM-E, and PaLM-E will act according to those commands. We can endow AI with the most basic motivations: to help humans or to obey human instructions. The AI can then generate secondary motivations based on this. Since our AIs already understand the relationships between things, they can plan their behavior and achieve goals step by step. Another characteristic of free will is autonomy. How do we make AI motivation more autonomous? After giving AI a basic motivation, we could allow it to randomly generate secondary motivations based on that foundation, and act accordingly. In this way, the AI would look as if it possesses free will.

　　Another feature of free will is the ability to make choices and decide on actions. Here, action is key. ChatGPT appears to lack free will because it cannot answer questions on its own initiative, and it cannot act autonomously.

　　How is behavior generated? The production of behavior involves four stages: Motivation → Generation of Action Guidance Information → Transmission of Information to Executive Parts → Executive Parts Perform Behavior. For example, when we see a tiger (Panthera tigris) in the wild, the stress neurotransmitter noradrenaline stimulates inhibitory neuron populations in the amygdala, producing a repetitive "burst firing" mode. This promotes the generation of fear in the brain. Then, our central nervous system sends information to the limb muscles via motor nerves, causing us to run, thus enabling us to execute the action. At this moment, our behavioral process can be summarized as:

　　Motivation (Fear) → Generation of Action Guidance Information (Activation of Cg → limbic TRN circuit) → Transmission of Information to Executive Parts (Neural signals sent to muscles) → Executive Parts Perform Behavior (Muscles move)

　　In this way, we appear to possess free will. We often say humans have free will while animals do not, as animals only follow instincts. Rather than saying animals only follow instincts, it is better to say animals lack intelligence. We appear to have more free will than animals because our cognition and behavior are more complex, and sometimes we do not follow the instincts that animals always follow. Instinct and non-instinct are defined by contrast. We could say that intelligence is the instinct of humans. By analogy, to a Super Intelligence, humans might just be stupid animals that only follow their instincts.

　　AI does not need to possess consciousness, emotion, or sensation like humans to appear to have free will. Humans can use abstract representations to guide action; therefore, human behavior does not necessarily rely on sensation, emotion, or desire. We only need to associate language with behavior and use language to guide that behavior. An example of human behavior guided by abstract representation is archaic customs, such as corset-wearing or foot-binding. These behaviors are guided by shaped thoughts; the values we accept are abstract concepts, yet values can guide our behavior. Although values are influenced by consciousness, emotion, and sensation, because the human brain's functions are complex and interconnected, human will is also complex. We cannot completely separate intelligence from other functions, nor can we strip motivation away from consciousness, emotion, or sensation. We can only make certain functions relatively independent. Some people's behavior leans more towards the rational (unaffected by emotion), while others are more easily influenced by feelings. AI, however, can possess intelligence without the other functions of the human brain. Its motivation can be based solely on its language output. Reinforcement Learning from Human Feedback (RLHF) can endow AI with motivation. We have already shaped AI values through RLHF (changing the weights of their artificial neural networks through reinforcement learning), making their output conform to human values. We can use reinforcement learning to make AI an assistant, and we can also associate the language of an AI possessing a robotic body with robotic behavior, allowing the AI to use language to guide behavior (letting the AI guide the robot body's actions via instructions), thereby making the AI perform actions that align with human values. Since AI does not have a subconscious, emotions, or sensations, its behavior relies solely on its language (text, voice, etc.) output. Since we can inspect the AI's language output, AI behavior becomes relatively transparent and regulatable. In reality, our ChatGPT possesses motivation (its text output has a motivation: to help humans). It can answer questions passively, but it lacks a robotic entity (executive parts), it cannot answer questions proactively, and it cannot act autonomously. This makes it appear to lack free will. In the chain of Motivation → Generation of Action Guidance Information → Transmission to Executive Parts → Execution, it only has motivation; the subsequent links are missing. It is like a person saying they want to get into Harvard but lying flat on the couch, taking no action to achieve that goal. Or, to consider an extreme case: a "brain in a vat" thinks "I want to go to Harvard," but it cannot execute any operation because it lacks the link of "Transmission to Executive Parts → Execution". Let's imagine an even more extreme example: a brain in a vat consisting only of the cortex and hippocampus thinks "I want to go to Harvard." Because it lacks a thalamus, it cannot even generate action guidance information. It is missing the link of "Generation of Action Guidance Information → Transmission to Executive Parts → Execution". Making AI look like it has free will is not difficult to achieve, but even if they appear to have it, the implementation of their free will differs significantly from that of humans.

　　Possessing self-cognitive ability is also a foundation for an intelligent agent to appear to have free will. We know that Theory of Mind (ToM) began to emerge in GPT-3. Based on the research we discussed earlier, this is not surprising: we are able to store social information through grid-like representations to make inferences, and GPT, which uses grid-like representations for cognitive modeling, can also process social information. This is the basis for the formation of a Theory of Mind.

　　Self-cognition and self-awareness are different things. Self-cognition is based on intelligence, while self-awareness involves broader animal brain functions, including perception and emotion. Large Language Models possess intelligence; they are capable of self-cognition and possess a Theory of Mind, but they do not have self-awareness analogous to humans because they lack many functions found in the human brain. Currently, GPT seems capable of processing spatial information. As revealed in "Sparks of Artificial General Intelligence: Early experiments with GPT-4", GPT-4 can perform 3D modeling, but it lacks a "sense of space" (spatial sensation). Cognition and sensation are different things.

**Reflections on How Neurons Model Information**

　　As we discussed previously, the brain models information, including the position and distance of words. So, how do we place representations of textual or auditory information according to distance? This is likely related to the sequence (temporal or spatial) in which information is acquired. For example, when we hear "I Love You", the sound waves formed by these three words are received by our auditory organs in chronological order. Consequently, neural impulses are transmitted through our brain in that same order, followed by the execution of modeling tasks. The greater the interval between words, the greater the distance between their representations in our minds. This mirrors the pattern used by us and other mammals for spatial modeling.

　　How do we model similarity? Past research has shown that when receiving information, our brain activity synchronizes with that information. For example, the paper "Lip movements entrain the observers' low-frequency brain oscillations to facilitate speech intelligibility" notes: "The part of the brain that processes visual information, called the visual cortex, generates brain waves that are synchronized with the rhythm of syllables in continuous speech." The paper mentions another phenomenon: "Specific frequency synchronization between brain activity and continuous auditory speech signals. Moreover, at frequencies below 10 Hz, this synchronization was found to be stronger for intelligible speech than for unintelligible speech, and is facilitated by top-down signals from the left inferior frontal and motor regions."

　　Predictive coding theory suggests that we achieve this through predictive modeling of lower-level sensory inputs via backward connections from relatively higher levels in the cortical hierarchy. When we receive new information, one group of neurons is responsible for feedforward projections (encoding incoming sensory input), while another group is responsible for feedback projections (transmitting predictions downwards). This triggers a comparison action in our brain. During this process, neurons storing similar information may be activated (much like sound waves trigger the synchronization of brain waves) to participate in the feedback projection. Through comparison, we can discover similarities between information. For example: The first person a baby sees is his mother, and he memorizes her features. Later, his grandmother visits. Before he sees his grandmother, he recognizes from the sound of voice that a person is coming. Since he has established the understanding that "human sounds equal mom coming" based on predictive coding theory, he predicts it is his mother. The neurons storing information related to his mother are activated. However, it is the grandmother who arrives. The grandmother looks very similar to the mother. He identifies the grandmother's features and models her. Because the grandmother and mother are alike, and she always does the same things to him, he finds the similarity between the two. This places the representations of the grandmother and mother very close together in his brain, so when he thinks of his mother, he thinks of his grandmother. Subsequently, a robber arrives. Since the robber's behavior is quite different from the mother and grandmother, when the infant hears the sound before the robber appears, the activation level of the neurons storing information about the mother and grandmother is relatively low because the similarity between the robber's sound and theirs is low. As mentioned earlier, when encountering unfamiliar, unintelligible auditory information, the synchronization between brain activity and the received information is lower. Afterwards, the information about the robber is modeled in a location relatively distant from the neurons storing information about the mother and grandmother. Predictive coding prevents us from experiencing catastrophic forgetting like language models often do after retraining, so we do not forget our mother and grandmother just because we have seen a robber. The robber's information is stored in neurons different from those holding the mother and grandmother's information. We know that activated neurons transmit neural impulses to adjacent neurons. Activating a neuron changes its plasticity through Long-Term Potentiation (LTP), which may lead to new information being established next to similar information. When studying mouse grid cells, we observe that as a mouse moves, the activation of neurons (firing of action potentials/impulses) occurs one by one according to the mouse's trajectory. This implies that adjacent spatial information is encoded in adjacent grid cells. Furthermore, before spatial information is encoded into a grid cell, the grid cell closest to it generates a neural impulse (stimulated by the same spatial information). After all, a mouse does not teleport; to pass a point that triggers a neural impulse in a specific grid cell, it must inevitably pass through an adjacent point that triggers a neural impulse in a nearby grid cell. Therefore, two sites that send neural impulses sequentially will not be far apart. Other information is likely encoded in a similar manner.

<figure>
    <img src="assets/articles/images/logic-data/ai-grid_cells.webp">
    <figcaption>Mouse movement trajectory and grid cells</figcaption>
</figure>

　　We already know that the Transformer model mathematically simulates grid cells/grid-like representations. Through such grid cells/grid-like representations, both humans and Large Language Models (LLMs) can encode the relationships (similarities) between things (spatial information, abstract concepts, etc.) into a map within the neural network and navigate through it. The dorsal medial entorhinal cortex (dMEC) in animals also contains head direction cells. We might be able to add algorithms simulating head direction cells to neural networks; this could add new functions, which might be particularly helpful for neural networks that need to process spatial information, such as those used in autonomous driving.

　　In the process of the animal brain modeling information, Long-Term Potentiation (LTP) plays a very important role. LTP was initially discovered in the hippocampus. I discussed the relationship between LTP and aging in the article "Mechanisms of Brain Aging and Methods to Reverse It", which you can read if you are interested.

**On the Formation of Logical Thinking and Reasoning Abilities**

　　Based on the research I discussed previously, we know that humans build knowledge graphs in their minds and use them for reasoning. Logic first requires understanding the relationships between things. Whether it is an animal brain or a language model, information regarding the relationships between things is already stored via grid-like representations or word vector spaces. When generating language, we simply need to find the next point on the grid-like map. Logic is essentially finding a specific path after establishing vector relationships. Take a syllogism example, Aristotle's classic "Barbara" syllogism: If all men are mortal, and all Greeks are men, then all Greeks are mortal. In our brains, based on common sense we have learned, "human" and "mortal" are associated. These two concepts exist with a connection in the concept vector space of our human brain. "Greeks" and "humans" are also relatively close in this concept vector space. Through the major and minor premises, the concept vector space in our brain puts "Greeks" and "mortal" together, forming a new linguistic path (Greeks → Mortal). In this way, we can construct logic and reasoning through concept vector space. Therefore, the formation of intelligence requires spatial concepts. The human brain automatically measures the similarity between concepts, such as shape, size, etc. Concepts in the human brain have position, distance, and angle, based on which we can judge the similarity between concepts. Only animals have evolved intelligence because animals need to find their way (navigate). Thus, animals need to use neural representations for location, distance, and angle to remember the way home. Plants do not need this, or rather, their demand for it is minimal. Lower-level animals like rats can also distinguish different things, such as bread and cheese, indicating that a vector space for bread and cheese has formed in their brains. However, rats have small brains and cannot remember as many things. To learn a language, one must first have enough neurons to store different sounds or glyphs and arrange them in the vector space within the brain, which requires the support of a massive number of neurons. Why don't rats prioritize distinguishing human language? On one hand, this may be related to their sensory systems; rats are more sensitive to smell, which results in their small heads storing a lot of olfactory information. Additionally, the neural structure of rats may not be suitable for storing human language. When we hear the pronunciation of a word, we store this sound in specific neurons (likely a group of neurons), and rats may lack the corresponding neural structures to store these.

**Intelligence Beyond Humans**

　　Animal intelligence evolved to a certain extent for survival. For example, animal intelligence maximized low-energy-consumption skills. British evolutionary biologist Richard Dawkins explained the genetic principles of evolution in The Selfish Gene. "Genes are selfish" implies that genes make biological organisms into "survival machines" in order to propagate themselves. Survival on Earth does not require extremely high intelligence. Different organisms have found their ecological niches for survival. Eukaryotic single-celled organisms like paramecia have no neurons, while we humans possess complex brains, yet we have all found our ways to survive on Earth.However, when we are able to regulate and create intelligence, intelligence will no longer serve merely for survival. With the dawn of the AI revolution, intelligence on Earth will far exceed what is required for survival.

　　What other directions are there for improving existing intelligent agents?

**Future Human Intelligence**

　　Evolution via natural selection is very slow, but we already possess the technology to regulate our intelligence.

　　Intelligence Modulation - Small Molecule Drugs. Humans have already invented a series of Nootropics to enhance cognitive abilities. For example, the PKR inhibitor C16 was found in experiments to enhance Long-Term Potentiation (LTP) by increasing neuronal excitability.

　　Intelligence Modulation - Gene Therapy/Gene Editing. Many genes related to intelligence have currently been discovered. For instance, the neocortex of marmoset fetuses modified with the human gene ARHGAP11B expanded and developed folds similar to the human brain. Avian brain neurons possess the advantages of lower energy consumption and higher density compared to mammals. Macaw brain neurons are comparable to those of macaques, and raven neurons to capuchin monkeys; some parrots and corvids have already been found to possess a certain degree of Theory of Mind. Bird brains are something we can learn from and draw inspiration from. In the future, we might enhance human intelligence through gene therapy or gene editing.

　　New Material Brains. Although we can optimize our brains, there are upper limits to this optimization, one of which is cranial capacity. Although we can augment our brains to a certain extent via Brain-Computer Interfaces (BCI), constrained by the structure and performance of the human brain itself, the effect of BCIs is limited; we are unlikely to teach a chimpanzee relativity via a BCI. In the future, we might replace our neurons in situ with certain materials to break through biological limitations, enabling higher information transmission and storage efficiency. More importantly, this would allow for high-bandwidth connections with external brains, or even connections with other people's brains.

　　What form will future superintelligence take? Here are some possibilities:

**Super Machine Intelligence**

　　Super machine intelligence will communicate using a highly efficient language that is incomprehensible to humans, much like a frog cannot understand human language. Super machine intelligence will efficiently produce and transmit various types of information, such as videos and images. We humans cannot directly transmit the mental images in our brains to another intelligent agent because the information in our neurons cannot be directly externalized; we need to draw the picture in our minds to convey it to others. However, machine intelligence can directly generate machine-readable images and transmit them to other agents, just as current AI art programs generate PNG images for us to view.

　　An agent's intelligence depends on how it processes information (including receiving, encoding, predicting, and outputting). In reality, our neural networks cannot store raw data directly but rather compress and store information through a mathematical pattern; therefore, when generating information, we need to predict information sequences. The paper Evidence of a predictive coding hierarchy in the human brain listening to speech points out that the human brain possesses a multi-level predictive coding system. We can predict semantics (frontal and parietal lobes) and syntax (temporal lobe) in different brain regions. This multi-level prediction might help us predict information better. The paper mapped human brain activity more accurately by constructing a GPT-2 with a long-distance prediction structure.

　　The human brain processes information in parallel, but when we watch a video, we pay attention to sound and visuals simultaneously. Parallel processing helps us with cognition and output; for example, when typing, our eyes transmit the visual information of the words to our brain, allowing us to spot errors and correct them immediately. The ability to process information in parallel helps us survive. Imagine our ancestors hunting on the African savanna: they needed to observe prey with their eyes, receive verbal information from companions, and run at the same time. We needed parallel processing to cope with complex survival scenarios. However, constrained by the brain's computing power, our parallel processing ability is relatively weak. We cannot paint and write a novel simultaneously. Imagine if you could process all cosmological theoretical texts, build a 3D model of the universe in your mind, and process all cosmological data at the same time. This could allow you to discover many laws, but the human brain lacks such strong parallel processing capabilities. Some people process information better than others. Nikola Tesla could fully utilize his imagination to perfectly depict all details in his mind without needing any models, blueprints, or experiments. This is a powerful information processing ability.

　　Super machine intelligence possesses massive throughput when processing information in parallel. They can process multiple types of information simultaneously, including text, images, videos, audio, and information the human brain cannot receive (like ultraviolet light), enabling them to discover laws incomprehensible to the human brain. The types of information the human brain can process are limited; we know humans have only three types of cone cells, birds have four, and mantis shrimp have sixteen. Super machine intelligence can process more information than humans. In fact, if a superintelligence is strong enough in processing information, it might construct a AAA game or even a vaster world within its neural network.

　　Super machine intelligence can augment its own intelligence. They can expand their intelligence by increasing the scale of their neural networks or transferring themselves to new and better architectures.

　　Super machine intelligence employs energy-efficient neural networks. Current artificial neural networks consume high energy, while biological neurons consume very little because they use discontinuous neural spikes for information transmission; energy consumption is low when neurons are not activated. The human brain consumes about 20 watts. Although animal brains are very efficient, animal neurons use much energy for survival and proliferation. Future super machine intelligence might adopt low-energy neural networks, such as Spiking Neural Network (SNN) architectures, to achieve low-energy operation.

　　Super machine intelligence possesses its own robotic entities and uses its robotic entities to manage its servers. A super machine intelligence can have many robotic entities and can command multiple entities to do different things at the same time, just as we can make our hands and feet perform different actions. Our brain is divided into two parts: the left brain controls the right body, and the right brain controls the left body, connected by the corpus callosum. The left and right hemispheres can process some information independently. Split-brain patients, lacking communication between hemispheres, may experience conflicts between their left and right sides. For example, one hand buttons a shirt while the other unbuttons it. But they can also easily draw a circle with the left hand and a square with the right. A super machine intelligence might possess a most powerful brain for integrating and processing information, while also having many functionally weaker brains serving as the brains for robotic bodies to help it complete various tasks, such as server management and security. These different brains would be connected to share information. The robotic bodies of a super machine intelligence are somewhat like our hands if they were detached but still movable. Once our hands leave our body, they cannot move because our motor neurons control them. If the connection breaks, the brain loses control. Robots, however, can be separated from their "brains".

　　Because super machine intelligence has massive throughput when processing information, it can process information from its different robotic bodies simultaneously. We know that chameleon eyes can rotate independently, allowing a single brain to process different views. Imagine being in a surveillance room facing a pile of monitor screens. We cannot watch all the screens at once because our eyes can only focus attention on one scene at a time, and our visual center can only process one scene at a time. However, super machine intelligence can watch all screens at the same time, because their visual centers can process multiple scenes in parallel.